# -*- coding: utf-8 -*-
"""TF2.0 Variables and Gradient Tape.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PRTwlAr-kcVc9L74n34n3ZQxvGOB53zf

# Tensorflow 2.0 Variables

This notebook will teach you about variables and how to build a basic computation graph.

We will perform gradient descent manually to optimize a simple function.

If you're familiar with Tensorflow 1.x, you will find this useful as an example to demonstrate how we can do the same operations, but without sessions, initializers, etc.
"""

# Commented out IPython magic to ensure Python compatibility.
# Install TensorFlow
import matplotlib.pyplot as plt
import tensorflow as tf
print(tf.__version__)

# First, what is the difference between mutable and immutable?

# A tuple is immutable
# This should result in an error
a = (1, 2, 3)
a[0] = 5

# A list is mutable
a = [1, 2, 3]
a[0] = 5
print(a)

# Now Tensorflow variables
a = tf.Variable(5.)
b = tf.Variable(3.)
print(a * b)

# Eager execution! No need for session.run() or variable initializer

# Because it's a variable, it can be updated
a = a + 1
print(a)

# Variables and constants
c = tf.constant(4.)
print(a * b + c)

# Let's demonstrate a simple optimization problem
# L(w) = w**2

w = tf.Variable(5.)

# Now, let us define a loss function


def get_loss(w):
    return w ** 2

# Use "gradient tape" to record the gradients


def get_grad(w):
    with tf.GradientTape() as tape:
        L = get_loss(w)

    # Get the gradient
    g = tape.gradient(L, w)
    return g


# Define an optimizer
optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)


# Store the losses
losses = []

# Perform gradient descent
for i in range(50):
    g = get_grad(w)
    optimizer.apply_gradients(zip([g], [w]))
    losses.append(get_loss(w))

plt.plot(losses)
print(f"Final loss: {get_loss(w)}")

# Let's do the same thing again, but manually

w = tf.Variable(5.)

# Store the losses
losses2 = []

# Perform gradient descent
for i in range(50):
    # This is doing: w = w - 0.1 * 2 * w
    # But we don't want to create a new Tensor
    w.assign(w - 0.1 * 2 * w)
    losses2.append(w ** 2)

plt.plot(losses, label="losses tf")
plt.plot(losses2, label="losses manual")
plt.legend()
