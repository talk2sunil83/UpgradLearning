{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Embedding\n",
    "\n",
    "![Word Embedding](https://cdn-images-1.medium.com/max/800/0*g24VvkPOJPaYDw6W.jpg)\n",
    "Photo Credit: https://cdn.pixabay.com/photo/2016/03/09/09/14/books-1245690_960_720.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Word Embedding is silver bullet to resolve many NLP problem. Most of modern NLP architecture adopted word embedding and giving up bag-of-word (BoW), Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA) etc. \n",
    "\n",
    "After reading this article, you will understand:\n",
    "- History of Word Embedding\n",
    "- Word Embedding Design\n",
    "- Apply off-the-shelf word embedding model\n",
    "- Embedding Visualization\n",
    "- Take Away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# History of Word Embedding\n",
    "Traditionally, we use bag-of-word to represent a feature (e.g. TF-IDF or Count Vectorize). Besides BoW, we can apply LDA or LSA on word feature. However, they have some limitations such as high dimensional vector, sparse feature. Word Embedding is a dense feature in low dimensional vector. It is proved that word embedding provides a better vector feature on most of NLP problem.\n",
    "\n",
    "In 2013, Mikolov et al. made Word Embedding popular. Eventually, word embedding is state-of-the-art in NLP. He released the word2vec toolkit and allowing us to enjoy the wonderful pre-trained model. Later on, gensim provide a amazing wrapper so that we can adopt different pre-trained word embedding models which including Word2Vec (by Google), GloVe (by Stanford), fastText (by Facebook).\n",
    "\n",
    "12 years before Tomas et al. introduces Word2Vec, Bengio et al. published a paper [1] to tackle language modeling and it is the initial idea of word embedding. At that time, they named this process as \"learning a distributed representation for words\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*FZVMHwCLO3fFo7FvMyA94Q.png)\n",
    "Capture from A Neural Probabilistic Language Model [2] (Benigo et al, 2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In 2008, Ronan and Jason [3] introduce a concept of pre-trained model and showing that it is a amazing approach for NLP problem. Word embedding became famous unitl Tomas released pre-trained model (Word2Vec) in 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*D6A44ZN5_zwTyuCAODM0fA.png)\n",
    "Capture from A Unified Architecture for Natural Language Processing [3] (Collobert & Weston, 2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Timeline:\n",
    "- 2001: Bengio et al. introduced a concept of word embedding\n",
    "- 2008: Ronan and Jason  introduced a concept of pre-trained model\n",
    "- 2013: Mikolov et al. released pre-trained model which is Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Design\n",
    "\n",
    "##### Low Dimensional\n",
    "![](https://food.fnr.sndimg.com/content/dam/images/food/fullset/2014/3/17/0/FNM_040114-KidsCake-rainbow-recipe_s4x3.jpg.rend.hgtvcom.616.462.suffix/1395082987380.jpeg)\n",
    "Photo Credit: https://www.foodnetwork.com/recipes/food-network-kitchen/four-layer-birthday-cake-3363221\n",
    "\n",
    "To tackle the high dimensional issue, word embedding use pre-defined vector space such as 300 to present every word. For demo purpose, I use 3 dimension to represent the following words:\n",
    "- Apple: [1.11, 2.24, 7.88]\n",
    "- Orange: [1.01, 2.04, 7.22]\n",
    "- Car: [8.41, 2.34, -1.28]\n",
    "- Table: [-1.41, 7.34, 3.01]\n",
    "\n",
    "As pre-defined the vector space (i.e. 3 in the above demo), number of dimension (or feature) is fixed no matter how large the corpus is. Comparing to BoW, number of dimension will be increased when unique word increase. Imagining we have 10k unique words in our documents, number of feature in BoW is 10k (without filtering high/ low frequency word) while the dimension can be keep as 3 in our demo.\n",
    "\n",
    "##### Semantic Relationship\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*oF1QyMamN5jXCXfffSRrqA.png)\n",
    "Photo Credit: https://gointothestory.blcklst.com/similar-but-different-c722f39d923d\n",
    "\n",
    "\n",
    "In general, the word vector encodes semantic relationship among words. It is a very important concept on word embedding as it benefits on tacking NLP problem. Word vectors will be closed if they have similar meaning. For example, buy and purchase will be closer. Unlike BoW, it only represent 0 or 1 (Counting having a word or not approach) and it cannot represent whether two wordings have similar meaning or not.\n",
    "\n",
    "In the above example, you may notice that Apple's vector and Orange's vector are closed than others meanwhile Apple's vector is far way from Car's vector relatively.\n",
    "\n",
    "##### Continuous bag-of-words (CBOW) & Skip-gram\n",
    "Mikolov et al proposed two new architectures [4] which reducing computation complexity and including additional context. \n",
    "CBOW is that using both n words before and after target word (w). For instance, \"the word vector encodes semantic relationship among words\". If the window (n) is 3, here is the subset of prediction list:\n",
    "- Case 1, Before Words: {Empty}, After Words: (word, vector, encodes), Predict Word: \"the\"\n",
    "- Case 2, Before Words: (the), After Words: (vector, encodes semantic), Predict Word: \"word\"\n",
    "\n",
    "Skip-gram uses the opposite approach which use the target word to predict n words before and after target word. For instance, \"the word vector encodes semantic relationship among words\". If the window (n) is 3, here is the subset of prediction list:\n",
    "- Case 1, Predict Word: \"the\", Words: (word, vector, encodes)\n",
    "- Case 2, Predict Word: \"word\", Words: (the, vector, encodes, semantic)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*QwiTOcVmwesADjQ3zMvSjA.png)\n",
    "Capture from Efficient Estimation of Word Representations in Vector Space (Tomas et al., 2013)\n",
    "Negative Sampling\n",
    "\n",
    "##### Negative Sampling\n",
    "Instead of leveraging all other words as negative label training records. Mikolov et al. proposed to use suitable small amount of negative training record to train the model. So that the whole operation become much faster. \n",
    "\n",
    "If you are not familiar with negative sampling, you may check out this article for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply off-the-shelf word embedding model\n",
    "Introduced history and model architecture, how can we use word embedding to tackle NLP problem?\n",
    "There are two approaches to deal with word embedding:\n",
    "- Leveraging off-the-shelf model\n",
    "- Building a domain specific model.\n",
    "\n",
    "This article will take the first approach. Selecting 3 well-known pre-trained models and leveraging gensim to load those model. Gensim, well known NLP library, already implement interface to deal with these 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/45/68e41b073b17c49dc9f02648acfd43b029072786a229465c27e9554c993e/tensorflow-2.4.0-cp37-cp37m-win_amd64.whl (370.7MB)\n",
      "Collecting opt-einsum~=3.3.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
      "Collecting wheel~=0.35 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
      "Collecting termcolor~=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting h5py~=2.10.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04a1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Collecting numpy~=1.19.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/a5/24db9dd5c4a8b6c8e495289f17c28e55601769798b0e2e5a5aeb2abd247b/numpy-1.19.4-cp37-cp37m-win_amd64.whl (12.9MB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting absl-py~=0.10 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/58/0aa6fb779dc69cfc811df3398fcbeaeefbf18561b6e36b185df0782781cc/absl_py-0.11.0-py3-none-any.whl (127kB)\n",
      "Collecting wrapt~=1.12.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Collecting gast==0.3.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting google-pasta~=0.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "Collecting protobuf>=3.9.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/f4/089025cfa3ee62f89cae73f4d36daf46f339c6df61becfe4b24f3aeb3c0d/protobuf-3.14.0-cp37-cp37m-win_amd64.whl (798kB)\n",
      "Collecting tensorboard~=2.4 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/83/179c8f76e5716030cc3ee9433721161cfcc1d854e9ba20c9205180bb100a/tensorboard-2.4.0-py3-none-any.whl (10.6MB)\n",
      "Collecting keras-preprocessing~=1.1.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "Collecting six~=1.15.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting flatbuffers~=1.12.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
      "Collecting astunparse~=1.6.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting grpcio~=1.32.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/5f/bf822211f7f94a2f6d0f8fd3bda3b804d7b24b6d5c84dbc6e6c9df4c74c2/grpcio-1.32.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/81/67/e2c34bb0628984c7ce71cce6ba6964cb29c418873847fc285f826e032e6e/google_auth_oauthlib-0.4.2-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/ef/24a91ca96efa0d7802dffb83ccc7a3c677027bea19ec3c9ee80be740408e/Markdown-3.3.3-py3-none-any.whl (96kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (41.0.1)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/fd/df1b3a59aca9537a187413651ad63b290d165559150a868a298fa837fe7b/google_auth-1.24.0-py2.py3-none-any.whl (114kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (0.15.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.22.0)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\" (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/da/d3c94fc7c72ad9298072681ec3e8cea86949acc5c4cce4290ba21f7050a8/cachetools-4.2.0-py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2019.6.16)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mukjain\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.5.1)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mukjain\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mukjain\\AppData\\Local\\pip\\Cache\\wheels\\b1\\c2\\ed\\d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: numpy, opt-einsum, wheel, termcolor, six, h5py, absl-py, wrapt, gast, google-pasta, protobuf, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, markdown, tensorboard-plugin-wit, tensorboard, keras-preprocessing, flatbuffers, astunparse, tensorflow-estimator, tensorflow\n",
      "  Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: astroid 2.2.5 requires typed-ast>=1.3.0; implementation_name == \"cpython\", which is not installed.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\mukjain\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-13d8b8c9dcc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglove2word2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglove2word2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "print('gensim Version: %s' % (gensim.__version__))\n",
    "\n",
    "class WordEmbedding:\n",
    "    __author__ = \"Edward Ma\"\n",
    "    __copyright__ = \"Copyright 2018, Edward Ma\"\n",
    "    __credits__ = [\"Edward Ma\"]\n",
    "    __license__ = \"Apache\"\n",
    "    __version__ = \"2.0\"\n",
    "    __maintainer__ = \"Edward Ma\"\n",
    "    __email__ = \"makcedward@gmail.com\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = {}\n",
    "        \n",
    "    def convert(self, source, ipnut_file_path, output_file_path):\n",
    "        if source == 'glove':\n",
    "            input_file = datapath(ipnut_file_path)\n",
    "            output_file = get_tmpfile(output_file_path)\n",
    "            glove2word2vec(input_file, output_file)\n",
    "        elif source == 'word2vec':\n",
    "            pass\n",
    "        elif source == 'fasttext':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "        \n",
    "    def load(self, source, file_path):\n",
    "        print(datetime.datetime.now(), 'start: loading', source)\n",
    "        if source == 'glove':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        elif source == 'word2vec':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "        elif source == 'fasttext':\n",
    "            self.model[source] = gensim.models.wrappers.FastText.load_fasttext_format(file_path)\n",
    "        else:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "            \n",
    "        print(datetime.datetime.now(), 'end: loading', source)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def get_model(self, source):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "            \n",
    "        return self.model[source]\n",
    "    \n",
    "    def get_words(self, source, size=None):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "        \n",
    "        if source in ['glove', 'word2vec']:\n",
    "            if size is None:\n",
    "                return [w for w in self.get_model(source=source).vocab]\n",
    "            else:\n",
    "                results = []\n",
    "                for i, word in enumerate(self.get_model(source=source).vocab):\n",
    "                    if i >= size:\n",
    "                        break\n",
    "                        \n",
    "                    results.append(word)\n",
    "                return results\n",
    "            \n",
    "        elif source in ['fasttext']:\n",
    "            if size is None:\n",
    "                return [w for w in self.get_model(source=source).wv.vocab]\n",
    "            else:\n",
    "                results = []\n",
    "                for i, word in enumerate(self.get_model(source=source).wv.vocab):\n",
    "                    if i >= size:\n",
    "                        break\n",
    "                        \n",
    "                    results.append(word)\n",
    "                return results\n",
    "        \n",
    "        return Exception('Unexpected flow')\n",
    "    \n",
    "    def get_dimension(self, source):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "        \n",
    "        if source in ['glove', 'word2vec']:\n",
    "            return self.get_model(source=source).vectors[0].shape[0]\n",
    "            \n",
    "        elif source in ['fasttext']:\n",
    "            word = self.get_words(source=source, size=1)[0]\n",
    "            return self.get_model(source=source).wv[word].shape[0]\n",
    "        \n",
    "        return Exception('Unexpected flow')\n",
    "    \n",
    "    def get_vectors(self, source, words=None):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "        \n",
    "        if source in ['glove', 'word2vec', 'fasttext']:\n",
    "            if words is None:\n",
    "                words = self.get_words(source=source)\n",
    "            \n",
    "            embedding = np.empty((len(words), self.get_dimension(source=source)), dtype=np.float32)            \n",
    "            for i, word in enumerate(words):\n",
    "                embedding[i] = self.get_vector(source=source, word=word)\n",
    "                \n",
    "            return embedding\n",
    "        \n",
    "        return Exception('Unexpected flow')\n",
    "    \n",
    "    def get_vector(self, source, word, oov=None):\n",
    "        if source not in ['glove', 'word2vec', 'fasttext']:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "            \n",
    "        if source not in self.model:\n",
    "            raise ValueError('Did not load %s model yet' % source)\n",
    "        \n",
    "        try:\n",
    "            return self.model[source][word]\n",
    "        except KeyError as e:\n",
    "            raise\n",
    "            \n",
    "            #TODO\n",
    "#             if oov is None:\n",
    "#                 raise\n",
    "            \n",
    "#             if 'not in vocabulary' in str(e):\n",
    "#                 if oov == ''\n",
    "\n",
    "    def build_visual_metadata(self, embedding, words, file_dir, \n",
    "                              metadata_name='metadata.csv', project_model_name='model.ckpt'):\n",
    "        # Create output directory if not exist\n",
    "        if not os.path.exists(file_dir):\n",
    "            os.makedirs(file_dir)\n",
    "\n",
    "        # Build graph\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "        embedding_graph = tf.Variable([0.0], name='embedding')\n",
    "        place = tf.placeholder(tf.float32, shape=embedding.shape)\n",
    "\n",
    "        set_embedding_graph = tf.assign(embedding_graph, place, validate_shape=False)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(set_embedding_graph, feed_dict={place: embedding})\n",
    "\n",
    "        # Build metadata\n",
    "        with open(os.path.join(file_dir, metadata_name), 'w') as f:\n",
    "            for word in words:\n",
    "                f.write(word + '\\n')\n",
    "\n",
    "        # Build projector\n",
    "        summary_writer = tf.summary.FileWriter(file_dir, sess.graph)\n",
    "        config = projector.ProjectorConfig()\n",
    "        embedding_conf = config.embeddings.add()\n",
    "        embedding_conf.tensor_name = 'embedding:0'\n",
    "        embedding_conf.metadata_path = metadata_name\n",
    "        projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "        # Save model\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, os.path.join(file_dir, project_model_name))\n",
    "\n",
    "        # Clear\n",
    "        sess.close()\n",
    "\n",
    "        \n",
    "downloaded_glove_file_path = '../text/stanford/glove/glove.6B.50d.txt'\n",
    "glove_file_path = '../text/stanford/glove/glove.840B.300d.vec'\n",
    "\n",
    "word2vec_file_path = '../text/google/word2vec/GoogleNews-vectors-negative300.bin'\n",
    "fasttext_file_path = '../text/facebook/fasttext/wiki.en.bin'\n",
    "\n",
    "word_embedding = WordEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to convert text file (downloaed from GloVe website) to vector format\n",
    "# word_embedding.convert(\n",
    "#      source='glove', ipnut_file_path=downloaded_glove_file_path, output_file_path=glove_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) is trained on google news and provided by Google. Based on 100 billion words from Google News data, they trained model with 300 dimensions.\n",
    "\n",
    "Mikolov et al. use skip-gram and negative sampling to build this model which is released in 2013.\n",
    "\n",
    "##### GloVe\n",
    "Global Vectors for Word Representation ([GloVe](https://nlp.stanford.edu/projects/glove/)) is provided by Stanford NLP team. Stanford provides various models from 25, 50 , 100, 200 to 300 dimensions base on 2, 6, 42,  840 billion tokens.\n",
    "\n",
    "Stanford NLP team apply word-word co-occurrence probability to build the embedding. In other word, if two words are co-exist many time, both words may have similar meaning so the matrix will be closer.\n",
    "\n",
    "##### fastText\n",
    "[fastText](https://fasttext.cc/) is released by Facebook which provides 3 models with 300 dimensions. One of the pre-trained model is trained with subword. For example, \"difference\", it will be trained by \"di\", \"dif\", \"diff\" and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding.load(source='word2vec', file_path=word2vec_file_path)\n",
    "word_embedding.load(source='glove', file_path=glove_file_path)\n",
    "word_embedding.load(source='fasttext', file_path=fasttext_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for source in ['glove', 'word2vec', 'fasttext']:\n",
    "    print('Source: %s' % (source))\n",
    "    print(word_embedding.get_vector(source=source, word='apple'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'word2vec'\n",
    "\n",
    "embedding = word_embedding.get_vectors(source=source)\n",
    "words = word_embedding.get_words(source=source)\n",
    "sub_embedding = embedding[:100000]\n",
    "sub_words = words[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Visualization\n",
    "One of state-of-the-art NLP is word embedding, what is it actually? It is a matrix and the simplest way is x and y coordinate but we have 300 dimensions not 2 dimensions. \n",
    "We can visualize it by using principal component analysis (PCA) or T-distributed Stochastic Neighbor Embedding (t-SNE). By leveraging TensorBoard, visualization can be presented easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding.build_visual_metadata(embedding=sub_embedding, words=sub_words, file_dir='./word_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*glwOAs3oK5IOOegT9ru7sw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    To start the tensorboard.\n",
    "    1. Open terminal\n",
    "    2. Go to parent directory of file_dir (e.g. parent directory of word_embedding)\n",
    "    3. execute \"tensorboard --logdir=word_embedding\" (e.g. the value of --logdir should be same \n",
    "        as what your provide in previous step)\n",
    "    4. Open browser to access http://localhost:6006 (depending on your host, the default port is 6006)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Away\n",
    "To access all code, you can visit my github repo.\n",
    "\n",
    "- Which off-the-shelf model should be use? Depending on your data, __it is possible that all of them are not useful for your domain specific data__.\n",
    "- Should we train word embedding layer base on your data? According to my experience, if you deal with __domain specific text and most of your word cannot be found from off-the-shell model__, you may consider to build customize word embedding layer. \n",
    "- Tensorboard picks first 100000 vectors due to browser resource concern. Recommend to pick a small portion of vectors by yourself.\n",
    "- Maximum model size of GloVe, Word2Vec and fasttext are ~5.5GB, ~3.5GB and ~8.2GB respectively. It takes about 9, 1, 9 minutes for GloVe, Word2Vec and fasttext respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- [1] Yoshua Bengio, Ducharme Rejean &Vincent Pascal. A Neural Probabilistic Language Model. 2001. https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf\n",
    "- [2] Yoshua Bengio, Ducharme Rejean, Vincent Pascal & Janvin Christian. A Neural Probabilistic Language Model. March 2003. http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "- [3] Collobert Ronan, & Weston Jason. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. 2008. https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf\n",
    "- [4] Tomas Mikolov, Greg Corrado, Kai Chen & Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. September 2013. https://arxiv.org/pdf/1301.3781.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
