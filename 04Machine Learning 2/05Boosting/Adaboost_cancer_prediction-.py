#!/usr/bin/env python
# coding: utf-8

# In this lab, youâ€™ll explore the breast cancer dataset and try to train the model to predict if the person is having breast cancer or not. We will start off with a weak learner, a decision tree with maximum depth = 2.
#
# We will then build an adaboost ensemble with 50 trees with a step of 3 and compare the performance with the weak learner.
#
# Let's get started by loading the libraries.

# In[1]:
import warnings
import os
import numpy as np
import pandas as pd
import sklearn
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.datasets import load_digits
from sklearn import metrics
%matplotlib inline

warnings.filterwarnings('ignore')
# We will use the breast cancer dataset in which the target variable has 1 if the person has cancer and 0 otherwise. Let's load the data.
pd.set_option('display.max_columns', None)
# In[2]:
cancer = load_breast_cancer()
digits = load_digits()
data = cancer
# In[3]:
df = pd.DataFrame(data=np.c_[data['data'], data['target']],
                  columns=list(data['feature_names']) + ['target'])
df['target'] = df['target'].astype('uint16')
# In[4]:
df
# In[5]:
df.head()
# In[6]:
# adaboost experiments
# create x and y train
X = df.drop('target', axis=1)
y = df[['target']]

# split data into train and test/validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)


# In[7]:
# check the average cancer occurence rates in train and test data, should be comparable
print(y_train.mean())
print(y_test.mean())


# In[8]:
# base estimator: a weak learner with max_depth=2
shallow_tree = DecisionTreeClassifier(max_depth=2, random_state=100)


# In[9]:
# fit the shallow decision tree
shallow_tree.fit(X_train, y_train)

# test error
y_pred = shallow_tree.predict(X_test)
score = metrics.accuracy_score(y_test, y_pred)
score


# Now, we will see the accuracy using the AdaBoost algorithm. In this following code, we will write code to calculate the accuracy of the AdaBoost models as we increase the number of trees from 1 to 50 with a step of 3 in the lines:
#
# 'estimators = list(range(1, 50, 3))'
#
# 'for n_est in estimators:'
#
# We finally end up with the accuracy of all the models in a single list abc_scores.

# In[14]:
# adaboost with the tree as base estimator
estimators = list(range(1, 50, 3))

abc_scores = []
for n_est in estimators:
    ABC = AdaBoostClassifier(base_estimator=shallow_tree, n_estimators=n_est, random_state=101)

    ABC.fit(X_train, y_train)
    y_pred = ABC.predict(X_test)
    score = metrics.accuracy_score(y_test, y_pred)
    abc_scores.append(score)


# In[15]:
plt.plot(estimators, abc_scores)
plt.xlabel('n_estimators')
plt.ylabel('accuracy')
plt.ylim([0.9, 1])
plt.show()

# %%
