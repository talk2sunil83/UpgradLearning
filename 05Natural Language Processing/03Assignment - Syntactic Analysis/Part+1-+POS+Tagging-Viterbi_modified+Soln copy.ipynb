{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech tagging using modified Viterbi\n",
    "\n",
    "Let's learn how to improve POS tagging of unknown words by modifying Viterbi Algorithm. Below is the structure of the notebook. \n",
    "\n",
    "1. Tagged Treebank corpus is available (Sample data to training and test data set)\n",
    "   - Basic text and structure exploration\n",
    "   \n",
    "2. Creating HMM model on the tagged data set.\n",
    "   - Calculating Emission Probabaility: P(observation|state)\n",
    "   - Calculating Transition Probability: P(state2|state1)\n",
    "   \n",
    "3. Developing algorithm for Viterbi Heuristic\n",
    "   - Model Prediction and Evaluation\n",
    "4. Modifying Viterbi to deal with unknown words\n",
    "\n",
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "wsj = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and validation\n",
    "random.seed(1234)\n",
    "train_set, validation_set = train_test_split(wsj,test_size=0.05)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of tagged words\n",
    "validation = [tup for sent in validation_set for tup in sent]\n",
    "len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "V = set(tokens)\n",
    "print(len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tags\n",
    "T = set([pair[1] for pair in train_tagged_words])\n",
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. POS Tagging Algorithm - HMM\n",
    "\n",
    "We'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word. \n",
    "\n",
    "To every word w, assign the tag t that maximises the likelihood P(t/w). Since P(t/w) = P(w/t). P(t) / P(w), after ignoring P(w), we have to compute P(w/t) and P(t).\n",
    "\n",
    "\n",
    "P(w/t) is basically the probability that given a tag (say Noun), what is the probability of it being w (say 'building'). This can be computed by computing the fraction of all NNs which are equal to w and dividing by the count of all tags t, i.e. \n",
    "\n",
    "P(w/t) = count(w, t) / count(t). \n",
    "\n",
    "\n",
    "The term P(t) is the probability of tag t, and we assume that a tag will depend only on the previous tag (first-order Markov assumption). In other words, the probability of a tag being Noun will depend only on the previous tag t(n-1). So for e.g. if t(n-1) is a ADJ, then t(n) is likely to be an Noun since adjectives often precede a noun (blue coat, tall building etc.).\n",
    "\n",
    "\n",
    "Given the penn treebank tagged dataset, we can compute the two terms P(w/t) and P(t) and store them in two large matrices (emission and transition probabilities respectively). The matrix of P(w/t) will be sparse, since each word will not be seen with most tags ever, and those terms will thus be zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# computing P(w/t) and storing in T x V matrix\n",
    "t = len(T)\n",
    "v = len(V)\n",
    "w_given_t = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examples\n",
    "\n",
    "# large\n",
    "print(\"\\n\", \"large\")\n",
    "print(word_given_tag('large', 'ADJ'))\n",
    "print(word_given_tag('large', 'VERB'))\n",
    "print(word_given_tag('large', 'NOUN'), \"\\n\")\n",
    "\n",
    "# will\n",
    "print(\"\\n\", \"will\")\n",
    "print(word_given_tag('will', 'VERB'))\n",
    "print(word_given_tag('will', 'NOUN'))\n",
    "\n",
    "# book\n",
    "print(\"\\n\", \"book\")\n",
    "print(word_given_tag('book', 'NOUN'))\n",
    "print(word_given_tag('book', 'VERB'))\n",
    "\n",
    "# He\n",
    "print(\"\\n\", \"He\")\n",
    "print(word_given_tag('He', 'NOUN'))\n",
    "print(word_given_tag('He', 'PRON'))\n",
    "\n",
    "# one\n",
    "print(\"\\n\", \"one\")\n",
    "print(word_given_tag('one', 'NOUN'))\n",
    "print(word_given_tag('one', 'NUM'))\n",
    "print(word_given_tag('one', 'ADP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examples\n",
    "print(t2_given_t1(t2='NOUN', t1='ADJ'))\n",
    "print(t2_given_t1('NOUN', 'DET'))\n",
    "print(t2_given_t1('NOUN', 'VERB'))\n",
    "print(t2_given_t1('.', 'NOUN'))\n",
    "print(t2_given_t1('VERB', 'NOUN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please note P(tag|start) is same as P(tag|'.')\n",
    "print(t2_given_t1('DET', '.'))\n",
    "print(t2_given_t1('VERB', '.'))\n",
    "print(t2_given_t1('NOUN', '.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    "\n",
    "tags_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags_df.loc['.', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the vanilla Viterbi based POS tagger\n",
    "\n",
    "Let's now use the computed probabilities P(w, tag) and P(t2, t1) to assign tags to each word in the document. We'll run through each word w and compute P(tag/w)=P(w/tag).P(tag) for each tag in the tag set, and then assign the tag having the max P(tag/w).\n",
    "\n",
    "We'll store the assigned tags in a list of tuples, similar to the list 'train_tagged_words'. Each tuple will be a (token, assigned_tag). As we progress further in the list, each tag to be assigned will use the tag of the previous token.\n",
    "\n",
    "Note: P(tag|start) = P(tag|'.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\r\n",
    "def Viterbi(words, train_bag = train_tagged_words):\r\n",
    "    state = []\r\n",
    "    T = list(set([pair[1] for pair in train_bag]))\r\n",
    "    \r\n",
    "    for key, word in enumerate(words):\r\n",
    "        #initialise list of probability column for a given observation\r\n",
    "        p = [] \r\n",
    "        for tag in T:\r\n",
    "            if key == 0:\r\n",
    "                transition_p = tags_df.loc['.', tag]\r\n",
    "            else:\r\n",
    "                transition_p = tags_df.loc[state[-1], tag]\r\n",
    "                \r\n",
    "            # compute emission and state probabilities\r\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\r\n",
    "            state_probability = emission_p * transition_p    \r\n",
    "            p.append(state_probability)\r\n",
    "            \r\n",
    "        pmax = max(p)\r\n",
    "        # getting state for which probability is maximum\r\n",
    "        state_max = T[p.index(pmax)] \r\n",
    "        state.append(state_max)\r\n",
    "        # print(key)\r\n",
    "    return list(zip(words, state))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of tagged words\n",
    "validation_run_base = [tup for sent in validation_set for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "validation_tagged_words = [tup[0] for sent in validation_set for tup in sent]\n",
    "print(len(validation_set))\n",
    "print(len(validation_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "validation_Viterbi1 = Viterbi(validation_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "print(len(validation_tagged_words))\n",
    "#print(test_run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(validation_Viterbi1, validation_run_base) if i == j] \n",
    "accuracy = len(check)/len(validation_Viterbi1)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases = [[validation_run_base[i-1],j] for i, j in enumerate(zip(validation_Viterbi1, validation_run_base)) if j[0]!=j[1]]\n",
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "test_cases_shared = 'Android is a mobile operating system developed by Google. Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013. Google and Twitter made a deal in 2015 that gave Google access to Twitter\\'s firehose. Twitter is an online news and social networking service on which users post and interact with messages known as tweets. Before entering politics, Donald Trump was a domineering businessman and a television personality. The 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years. This is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe. Show me the cheapest round trips from Dallas to Atlanta. I would like to see flights from Denver to Philadelphia. Show me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco. NASA invited social media users to experience the launch of ICESAT-2 Satellite.'\n",
    "test_words = word_tokenize(test_cases_shared)\n",
    "\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagged_seq)\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification -1: tags of all unknown words are replaced by Noun (Noun being the most common tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modification -1: all unknown words are replaced by Noun (Noun being the most common tag)\n",
    "\n",
    "def Viterbi_modf1(test_words, train_bag = train_tagged_words):\n",
    "    tagged_seq = Viterbi(test_words)\n",
    "    V = list(set([pair[0] for pair in train_bag]))\n",
    "    \n",
    "    words = [pair[0] for pair in tagged_seq]\n",
    "    Viterbi_tags = [pair[1] for pair in tagged_seq]\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        if word not in V:\n",
    "            Viterbi_tags[key] = 'NOUN'\n",
    "            \n",
    "    \n",
    "    return list(zip(words, Viterbi_tags))           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_Viterbimodf1 = Viterbi_modf1(validation_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check_modf1 = [i for i, j in zip(validation_Viterbimodf1, validation_run_base) if i == j] \n",
    "accuracy = len(check_modf1)/len(validation_Viterbimodf1)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tagged_seq2_test = Viterbi_modf1(test_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "tagged_seq2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification -2: Rules-based algorithm for unknown words and making it work in tandem with Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viterbi Modification -2: \n",
    "## 1. all unknown words with first letter capital/ all letters capitals are tagged as Noun, numbers are tagged as NUM, words ending with '-ous' as ADJ, and rest as Noun\n",
    "\n",
    "def Viterbi_modf2(test_words, train_bag = train_tagged_words):\n",
    "    tagged_seq = Viterbi(test_words)\n",
    "    V = list(set([pair[0] for pair in train_bag]))\n",
    "    \n",
    "    words = [pair[0] for pair in tagged_seq]\n",
    "    Viterbi_tags = [pair[1] for pair in tagged_seq]\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        if word not in V:\n",
    "            ## word ending with '-ous'\n",
    "            if word[-3:] == 'ous':\n",
    "                Viterbi_tags[key] = 'ADJ'\n",
    "            \n",
    "            ## if word is number\n",
    "            elif (word.isdigit() == True or word[:-2].isdigit() == True):\n",
    "                Viterbi_tags[key] = 'NUM'\n",
    "                \n",
    "            ## all letters capitalised\n",
    "            elif word.upper() == word:\n",
    "                Viterbi_tags[key] = 'NOUN'\n",
    "                \n",
    "            ## first letter is capitalised:\n",
    "            elif word[0].upper() == word[0]:\n",
    "                Viterbi_tags[key] = 'NOUN' \n",
    "                \n",
    "            else: \n",
    "                Viterbi_tags[key] = 'NOUN'\n",
    "    \n",
    "    return list(zip(words, Viterbi_tags))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_Viterbimodf2 = Viterbi_modf2(validation_tagged_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check_modf2 = [i for i, j in zip(validation_Viterbimodf2, validation_run_base) if i == j] \n",
    "accuracy = len(check_modf2)/len(validation_Viterbimodf2)\n",
    "\n",
    "accuracy\n",
    "\n",
    "## changing the code of Num to incorporate st, th, nd improved the accuracy by 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tagged_seq3_test = Viterbi_modf2(test_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "tagged_seq3_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Modification -3: State probability is dependent only on transition probability for the unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Modification -3: state probability is dependent only on transition probability\n",
    "def Viterbi_modf3(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            \n",
    "            if word in V:\n",
    "                state_probability = transition_p * emission_p              \n",
    "            else:\n",
    "                state_probability = transition_p\n",
    "            \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "        print(key)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "validation_Viterbimodf3 = Viterbi_modf3(validation_tagged_words)\n",
    "end = time.time()\n",
    "difference1 = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(validation_Viterbimodf3, validation_run_base) if i == j] \n",
    "\n",
    "accuracy = len(check)/len(validation_Viterbimodf3)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tagged_seq3_test = Viterbi_modf3(test_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "tagged_seq3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}